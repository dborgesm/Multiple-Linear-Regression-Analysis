---
title: "Bike Sharing Analysis"
author: "Daniela Borges"
date: "31/12/2019"
output: html_document
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)# fig.height = 8, fig.width = 10) 
```

```{r setup q1.1,include=FALSE}
#install.packages("lmtest")
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(lmtest))
suppressPackageStartupMessages(library(stats))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(pander))
suppressPackageStartupMessages(library(faraway))
suppressPackageStartupMessages(library(ISLR))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(sqldf))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(png))
suppressPackageStartupMessages(library(grid))
```

\newpage
## Introduction

Bike-sharing systems transform the traditional bike rental process where obtaining membership, rental and bike returned have been automated via a network of bike stations. These systems allowed the riders to take the bike at any bike station and returned it to the closest one to their destination. Today, there exists plenty of interest in these systems because most cities are highly populated, and the implementation of these benefits the traffic, environmental, and health issues.


The objective of the project was to perform a multiple linear regression analysis to predict the number of registered members that used this bike-sharing system, and identify behaviors depending on the environmental and seasonal settings. For instance, weather conditions, precipitation, and other factors could affect rental behavior. Performing this analysis would help to understand better the trends depending on the season and the relationship between the total number of registered users and the variables of interests.

\newpage

## Data Description and Exploratory Analysis
### Variable Description

The data of the project was provided by Capital Bikeshare, a subscription-based bike-sharing company, located in Washington DC from 2011 to 2012.  This data consisted of weather condition factors and if the day of registration was a working day or not. Therefore, the variables involved in the data were the following. 

* **Instant**. Record Index
* **Date**. Date when the rider rented the bike
* **Season**. Season of the year when the bike was rented
    + Spring, summer, fall, and winter
* **Year**. Year when the survey was done
* **Month**. The month when the bike was rented, from January to Decemeber 
* **Holiday**. Specifies whether the current date when the bike was rented was a holiday or not
* **Week day**. Day of the week the bike was rented, from Sunday to Saturday
* **Working day**. Specifies whether the bike was rented on a working day or not
* **Weather sit**. Characteristics of the weather when the bike was rented
    + Clear, few clouds or partly cloudy
    + Mist and cloudy; mist and broken clouds; mist and few clouds; mist
    + Light snow; light rain with thunderstorm and scattered clouds; light rain and scattered clouds
    + Heavy rain with ice pallets thunderstorm and mist; snow and fog
* **Temperature**. Normalized temperature in Celsius in hourly scale when the bike was rented.
* **Feels like temperature**. The normalized feeling temperature in Celsius on hourly scale when the bike was rented. 
* **Humidity**. Normalized humidity when the bike was rented.
* **Wind speed**. Normalized wind speed when the bike was rented.
* **Registered**. Total of riders that rented a bike which membership hast the following characteristics: annual member, 30-day member or day key member 


Variable  | Name      | Values                      | Type
----------|-----------| ----------------------------  | -----
Instant   | instant   | [1, .., 731]                  | int
Date      | dteday    | dd/mm/yyyy | object
Season    | season    | [1, 2, 3, 4] | categorical
Year      | yr        | [2011, 2012] | binary
Month     | mnth      | [1, ...,12]   | categorical
Holiday   | holiday   | [0,1] | binary
Week day  | weekday   | [0, ..., 6]| categorical
Working day | workingday | [0,1] | binary
Weather sit| weathersit | [1,2,3] | categorical 
Temperature | temp | (0,1) | float
Feels like temperature | atemp | (0,1) | float
Humidity | hum | (0,1) | float
Wind speed | windspeed | (0,1) | float
Registered | registered | [20, ..., 246] | int 

After performing a descriptive analysis of the data, it was decided that the instant and date were not necessary as predictors.The reason for this was due to that instant was just a record and other variables could represent the date. Futhermore, the continuous  variables were already normalized so there was no need to do it to perform the modelling. 

```{r setup q1.2, include=FALSE}
bike = read.csv("/Users/ddbor/Documents/MDA/STATS 9159A (Statistic Modelling I)/Assigments/Project/day.csv")

# The features instant, dteday, casual and cnt will be removed from the model 
bike = select (bike,-c(instant,dteday,casual,cnt))

# Column names
cname = names(bike)
```

In this step, a correlation matrix would be used to understand the variables and assessed the existing relationship between them with the help of a correlation matrix (Figure 1).

Relevant outcomes from the correlation matrix were as follows: 

* atemp and temp variables are highly correlated, meaning that one can be explained by the other. Therefore, only the most correlated with the objective variable would be kept
* season and month show a linear relationship, which means that they are highly correlated, and we need remove one of them later
* The scatter plot between registered and weekday does not show significance change, so weekday should be removed
* Feels like temperature and month have a relationship due to the number of users registered behavior can change depending on the month and the temperature

```{r setup q1.3, fig.width = 9, fig.height = 7, echo=FALSE}
pairs(~.,data=bike, main="Correlation Matrix")

```


\newpage

Now, the relationship between the registered users and the feels like temperature during each season (Figure 2) shows that users' registration tends to peak during the summer and decreases during the winter.

```{r setup q1.4, fig.width = 5, fig.height = 3,fig.align='center',echo = FALSE}

m = which(bike$mnth<= 6)
monthdata = bike[m,]

ggplot(bike, aes(season, registered)) + geom_point(position = position_jitter(w = 1, h = 0), aes(color = atemp)) + scale_color_gradient(low = "#88d8b0", high = "#ff6f69") + labs(title="Registered vs Season", caption = "Figure 2", x = "Season", y = "Registered") + 
  theme(plot.title = element_text(hjust = 0.5))
```


Additionally, Figure 3 shows that people register more when the weather conditions are better which also reafirms that during the winter season this number tends to decrease. 
```{r setup q1.5, fig.width = 5, fig.height = 3,fig.align='center',echo = FALSE}
bike$mnth = as.factor(bike$mnth)
bike$weathersit = as.factor(bike$weathersit)
bike$season = as.factor(bike$season)

weather_summary_by_season <- sqldf('select weathersit, season, avg(registered) as registered from bike group by weathersit, season')

ggplot(bike, aes(x=season, y=registered, color=weathersit))+geom_point(data = weather_summary_by_season, aes(group = weathersit))+geom_line(data = weather_summary_by_season, aes(group = weathersit))+ggtitle("Bikes Rent By Weather")+ scale_colour_hue('Weather',breaks = levels(bike$weathersit), labels=c('Good', 'Normal', 'Bad')) + labs(title="Registered vs Season", caption = "Figure 3", x = "Season", y = "Registered") + theme(plot.title = element_text(hjust = 0.5))

```


The variables that were removed before modelling were "temperature", "month" and "weekday", they could lead to multicollinearity and innacurate estimations. It was decided to keep "season" in the model instead of "month", because they had high correlation and season explained more the objective variable and had smaller number of categries in it. Thus, because month was removed, including a quadratic term might no be necessary because it is the only one that show a curvy pattern with some of the other variables. As for "weekday", its scatterplot between the objective variable did not show signigicant changes and could be represented with the working day variable. 


## Model Description

### Methodology

First, the data was splited into around 70% - 30% between training and testing stages. The model learned on the training data in order to be generalized in the test set. Second, a linear regression model using lasso as penalization was used to give an idea of which predictors were important and which could be removed. Afterward, different models were fitted, and cross-validation was used to evaluate its performance using MSE as criteria. Subsequently, the assumptions of the model selected in the last step were verified; these were linearity, equal variance, and normality. In this step, residual tests and diagnostic plots were performed. Also, influential points and outliers were analyzed. Finally, model validation was used on the test set, using the model with the best performance


### Model Building 

Lasso regression was performed in the full first-order model, and it was found that all the coefficients were necessary for the model (different than zero). Furthermore, the results indicated that penalizing the model increased the mean squared error (Figure 4). Hence, the first-order model with no penalization was preferable.

```{r setup q1.6, echo = FALSE}
# Let's separate our data in training and testing data and convert month and weathersit as factor. 
bike =select(bike, -c(temp, mnth, weekday))
#bike$mnth = as.factor(bike$mnth)
bike$weathersit = as.factor(bike$weathersit)
set.seed(0)
n = nrow(bike)
idx_tr <- sample(n,500,replace=FALSE)

biketr <- bike[idx_tr,]
bikets <- bike[-idx_tr,]
```

```{r set up q1.7, fig.width = 4, fig.height = 3, fig.align='center',echo=FALSE}
X_tr = model.matrix(registered~.,biketr)[, -1] #the first column (for intercept) is eliminated
y_tr = biketr$registered

X_ts = model.matrix(registered~.,bikets)[, -1] #the first column (for intercept) is eliminated
y_ts = bikets$registered

fit_lasso_cv = cv.glmnet(X_tr, y_tr, alpha = 1, lambda = exp(seq(from = -7, to = 7, by = 0.1)))


bestlam1 = fit_lasso_cv$lambda.min
fit_lasso_best = glmnet(X_tr, y_tr, alpha = 1, lambda = bestlam1)
df1 = as.data.frame(coef(fit_lasso_best)[,1], stringsAsFactors=FALSE)
names(df1) = "Coefficients"

#pander(df1)
plot(fit_lasso_cv)

title(sub ="Figure 4",
      cex.main = 2,   font.main= 4,
      cex.sub = 0.75, font.sub = 3,
      col.lab ="darkblue"
      )

```

After performing lasso, it was decided to explore three models besides the original one, these three included interactions because one of the objectives was to explain how the relationships between weather conditions and seasons with the other factors affected the total number of users' registrations. The models were explained in Table 2.

Model | Characteristics
------|-----------------
Quadratic Model | This model included all the quadratic terms of each predictors and all the interactions between each one of them
Quadratic Model (Lasso) | This model included all the variables with non zero coefficients after performing lasso in the Quadractic Model
Interaction Model | This model included only the original predictors plus interactions between different predictors
Full model | This model included the original predictors

Table: Models Fitted


Cross-validation was used to evaluate the performance of each model using 5 folds and Root Mean Square Error (MSE) as criteria (Table 3). The results indicated that the best model was the OLS model because it had the smaller MSE. 
```{r set up q1. 8, echo=FALSE}
# Four models were fitted and the performance were evaluated using cross validation
# Fitted models:
# Model 1: Model with interactions and Quadratic Models
# Model 2: Model after performing lasso in Model 1
# Model 3: Model with interactions
# Model 4: Full Model 

set.seed(251082976)
train.control = trainControl(method="cv", number = 5)

# Model with original predictors plus only interactions between different predictors
model_interaction = train(registered~(season + yr + workingday + 
                                         holiday + weathersit + atemp + 
                                         hum + windspeed)^2, 
                           data=biketr, method="lm", trControl=train.control)

# Model that includes the original predictors, interactions and quadratic terms. 
model_quad = train(registered~(season+yr+holiday+workingday+weathersit+atemp+hum+windspeed)^2+
                     I(atemp^2)+I(hum^2)+I(windspeed^2), 
                   data=biketr, method="lm", trControl=train.control)

# Model after performing lasso in the quadratic model 
model_quad_lasso = train(registered~season+yr+holiday+workingday+weathersit+atemp+hum+windspeed+
  I(atemp^2) +I(hum^2)+season:yr+season:holiday + season:workingday +
  season:weathersit + season:atemp + season:hum + season:windspeed + 
  yr:holiday + yr:workingday + yr:weathersit + yr:atemp + holiday:weathersit + 
  workingday:weathersit + workingday:atemp + workingday:windspeed + weathersit:hum + weathersit:windspeed + hum:windspeed, data=biketr, method="lm", trControl=train.control)

# Model with the original predictors. 
fullmodelcv = train(registered~., 
                           data=biketr, method="lm", trControl=train.control)

model_quadratic = mean((model_quad$resample$RMSE)^2)
model_quadlasso = mean((model_quad_lasso$resample$RMSE)^2)
interaction_model = mean((model_interaction$resample$RMSE)^2)
fullmodelcvmse = mean((fullmodelcv$resample$RMSE)^2)

df2 = data.frame("Quadratic Model" = model_quadratic,
           "Quadratic Model (Lasso)"= model_quadlasso, 
           "Interaction Model" = interaction_model, 
           "Full Model" = fullmodelcvmse, check.names = FALSE)

pander(pandoc.table(df2, caption="MSE"))
```


```{r set up q1.9, fig.width = 4, fig.height = 3, fig.align='center',echo=FALSE, include=FALSE}

# Lasso was performed to eliminate some variables from the quadractic model specified above. 

X_trq = model.matrix(registered~(season+yr+holiday+workingday+weathersit+atemp+hum+windspeed)^2+
                     I(atemp^2)+I(hum^2)+I(windspeed^2),
                     biketr)[, -1] #the first column (for intercept) is eliminated
y_trq = biketr$registered


fit_lasso_cvq = cv.glmnet(X_trq, y_trq, alpha = 1, lambda = exp(seq(from = -7, to = 7, by = 0.1)))


bestlam1q = fit_lasso_cv$lambda.min
fit_lasso_bestq = glmnet(X_tr, y_tr, alpha = 1, lambda = bestlam1)
df1q = as.data.frame(coef(fit_lasso_best)[,1], stringsAsFactors=FALSE)
names(df1q) = "Coefficients"

#pander(df1)
plot(fit_lasso_cvq)

title(sub ="Figure 4",
      cex.main = 2,   font.main= 4,
      cex.sub = 0.75, font.sub = 3,
      col.lab ="darkblue"
      )


```
The results of the previous analysis showed that the simple linear regression model had the best performance overall. Because of this, it was chosen to model the data, and would be used to make future predictions.

### Model Fitting

After fitting the full model, the majority of the predictors were significant (Table 5); this is because their p-values were less than the significance level of 5%. Furthermore, holiday had a p-value higher than the significance level; however, it was left in the model because its value was not significantly higher. The $Adjusted R^2 = 0.8513$ indicated that this model explained most of the variability of the data considering the number of predictors. 


From the coefficients, it was expected that people tended to use the service less during adverse weather conditions(thunderstorm, light rain, or strong wind speed) or holidays, because of the negative coefficients. These means that each unit change in these variables would decrease the number of registrations.

```{r set up q1. 10, echo=FALSE}
# Fitting the full model using the training data with all the remaining variables

fullmodel = lm(registered~., data=biketr)
pander(summary(fullmodel), caption="Full Model")
#anova(fullmodel, model_int)
```

### Collinearity

The collinearity of the model was verified by lookig at the variance inflation value (VIF). Based on a VIF cut off equal to 10, because none of the predictors had a higher VIF than the cut off, there was no severe collinear issue between the predictors, hence the variance of the estimates were not inflated, hence the p-values can be trusted and that means that the true values of the predictors were closer to the estimated coefficients. 

```{r set up q1. 11, echo=FALSE, include=FALSE}
# Calculating the VIF values 

pander(vif(fullmodel), caption="VIF")
```

### Model Assumptions

#### Equal Variance (EV) and Linearity. 

On one side, the EV of the model was violated. The residual plot showed that the distribution belt of points became wider along with x-axis. Also, the Breusch-Pagan test p-value was smaller than the significance level, which confirmed the violation of the EV. On the other side, the linearity assumptions was met, the residual plot did not show a pattern different than linear.

```{r set up q1. 12,fig.width = 4, fig.height = 3, fig.align='center', echo=FALSE}

#Let's check the model assumptions
plot(resid(fullmodel)~fitted(fullmodel), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals", sub="Figure 5",
     cex.lab=.8, cex.axis=.5, cex.main = 0.8, cex.sub =0.75)
abline(h = 0, col = "darkorange", lwd = 2)

bpval = bptest(fullmodel)$p.value

```

|Test          |  P-value |
|------------- | -------| 
|Breusch-Pagan | `r bpval`|

Table: Equal Variance Test

\newpage
#### Normality 

The normality assumption was verified using a Q-Q plot and the Shapiro Wilk test. The Q-Q plot revealed that the distribution had a heavy left tail, and the p-value of the test was lower than the significance level. Both of these confirmed the violation of the normality assumption.

```{r set up q1. 13, fig.width = 4, fig.height = 3, fig.align='center',echo=FALSE}

#Let's check the model assumptions
qqnorm(resid(fullmodel), main = "Normal Q-Q Plot", col = "darkgrey",
       cex.lab=.8, cex.axis=.5, cex=0.8,  sub="Figure 6",
     cex.lab=.8, cex.axis=.5, cex.main = 0.8, cex.sub =0.75)
qqline(resid(fullmodel), col = "dodgerblue", lwd = 2)

spval = shapiro.test(resid(fullmodel))$p.value
```

|Test          |  P-value |
|------------- | -------| 
|Shapiro-Wilk  | `r spval`|
Table: Normality Test

```{r set up q1. 14, echo=FALSE, include=FALSE}

#Let's check the model assumptions
par(mfrow=c(1,2), pin=c(3,2))
plot(resid(fullmodel)~fitted(fullmodel), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals",
     cex.lab=.8, cex.axis=.8,)
abline(h = 0, col = "darkorange", lwd = 2)
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(fullmodel), main = "Normal Q-Q Plot", col = "darkgrey",
       cex.lab=.8, cex.axis=.8,)
qqline(resid(fullmodel), col = "dodgerblue", lwd = 2)
```

```{r set up q1. 15, echo=FALSE, include=FALSE}
# Test for the EV and Normality assumption 
# With this model only the linearity seems ok the qq plot shows heavy left tail... 

bpval = bptest(fullmodel)$p.value
spval = shapiro.test(resid(fullmodel))$p.value
```

#### Stabilizing the Model Assumptions

The Box-Cox Transformation was used to stabilized the E.V. and Normality, the lambda used to transformed the registered variable was 0.85, which was the most close to the real value of the lambda. Neverthless, the assumptions were still violated. 

```{r set up q1. 16, echo=FALSE, include=FALSE}
# Looking at the Box-Cox Curve to estimate the alpha and see if the transformation stabilizes the EV and Normality assumptions. 

boxcox(fullmodel,lambda = seq(0.7, 1, by = 0.005))
lambda = 0.85

# The cox-box transformation did not improved the model
# Hence this will be one of the limitations of the model

lm_cox <- lm(((registered^(lambda)-1)/(lambda)) ~., data = biketr)
par(mfrow=c(1,2), pin=c(3,2))
plot(resid(lm_cox)~fitted(lm_cox), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals",
     cex.lab=.8, cex.axis=.8,)
abline(h = 0, col = "darkorange", lwd = 2)
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(lm_cox), main = "Normal Q-Q Plot", col = "darkgrey",
       cex.lab=.8, cex.axis=.8,)
qqline(resid(lm_cox), col = "dodgerblue", lwd = 2)


# Model interactions
# The EV of this model looks better however the Normality Q-Q plot was still violated...
par(mfrow=c(1,2), pin=c(3,2))
plot(resid(model_interaction)~fitted(model_interaction), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals",
     cex.lab=.8, cex.axis=.8,)
abline(h = 0, col = "darkorange", lwd = 2)
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model_interaction), main = "Normal Q-Q Plot", col = "darkgrey",
       cex.lab=.8, cex.axis=.8,)
qqline(resid(model_interaction), col = "dodgerblue", lwd = 2)

#bpinter = bptest(model_interaction)$p.value
spvalinter = shapiro.test(resid(model_interaction))$p.value


# Quadratic Model
# The EV and Normality were still violated... 
par(mfrow=c(1,2), pin=c(3,2))
plot(resid(model_quad)~fitted(model_quad), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals",
     cex.lab=.8, cex.axis=.8,)
abline(h = 0, col = "darkorange", lwd = 2)
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model_quad), main = "Normal Q-Q Plot", col = "darkgrey",
       cex.lab=.8, cex.axis=.8,)
qqline(resid(model_quad), col = "dodgerblue", lwd = 2)

#bpinter = bptest(model_quad)$p.value
spvalinter = shapiro.test(resid(model_quad))$p.value

# Quadratic Model After Lasso
# The EV and Normality were still violated
par(mfrow=c(1,2), pin=c(3,2))
plot(resid(model_quad_lasso)~fitted(model_quad_lasso), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals",
     cex.lab=.8, cex.axis=.8)
abline(h = 0, col = "darkorange", lwd = 2)
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model_quad_lasso), main = "Normal Q-Q Plot", col = "darkgrey",
       cex.lab=.8, cex.axis=.8,)
qqline(resid(model_quad_lasso), col = "dodgerblue", lwd = 2)

#bpinter = bptest(model_quad_lasso)$p.value
spvalinter = shapiro.test(resid(model_quad_lasso))$p.value

```

#### Influential Points

The outliers shown in Figure 5 were detected by calculating the absolute value of the standardized residuals greater than two, and the influential points were calculated using Cook's distance. If the influential points were considered as errors and were removed from the model, the EV and normality assumptions were still violated. However, by analyzing the data, there was no reason for them to be removed, and they did not look severe to change the model significantly, hence they were kept. 

```{r set up q1. 17, echo=FALSE}

# Cook's distance

# The last obs is an influential point
inf = which(cooks.distance(fullmodel) > 4 / length(cooks.distance(fullmodel)))
con = cooks.distance(fullmodel) > 4 / length(cooks.distance(fullmodel))
par(mfrow=c(1,2), pin=c(3,2))
plot(resid(fullmodel)~fitted(fullmodel), col = factor(abs(rstandard(fullmodel)) > 2), pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Outliers ",  sub="Figure 7",
     cex.lab=.8, cex.axis=.5, cex.main = 0.8, cex.sub =0.75)

abline(h = 0, col = "darkorange", lwd = 2)
title(sub ="Figure 7",
      cex.main = 2,   font.main= 4,
      cex.sub = 0.75,
      col.lab ="darkblue"
      )
plot(resid(fullmodel)~fitted(fullmodel), col = factor(con), pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Influential Points",  sub="Figure 8",
     cex.lab=.8, cex.axis=.5, cex.main = 0.8, cex.sub =0.75,
     colors = "blue")

abline(h = 0, col = "darkorange", lwd = 2)
title(sub ="Figure 8",
      cex.main = 2,   font.main= 4,
      cex.sub = 0.75,
      col.lab ="darkblue"
      )

# bptest
bpval1 = bptest(fullmodel)$p.value

# Shapiro test
spval1 = shapiro.test(resid(fullmodel))$p.value

```


\newpage

## Results

The model was applied to the test set and the resulted MSE was 44% higher compared to the training set, thus it could be infered that the selected model was overfitting. Looking at the mean absolute error (MAE) averagely our model predictions were off by approximately 522 registered users in comparison to the average of the test data. Hence, it can be concluded that the predictions were very accurate, because they were close to the actual values.

```{r set up q1. 18, echo=FALSE}
# Applying the model to the test data

pred_ls = predict(fullmodel, newdata=bikets) # prediction for test data
mse_ls = mean((pred_ls-bikets$registered)^2)

df3 = data.frame("Prediction" = pred_ls,
           "Registered"= bikets$registered)

mae = mean(abs(pred_ls-bikets$registered))
average = mean(bikets$registered)
```
MSE        |       MAE     | $\overline{Y}$ |
--------   | --------------| ---------------|
`r mse_ls` | `r round(mae,3)`       | `r round(average,3)`|
Table: Performance Metrics

The coefficients included in the final model were year, the feels like temperature, working day, season, holiday, weather sit, humidity and wind speed. Looking at the coefficient path the most important predictors of this model were season, year, working day and weather sit because they did not converge to zero as fast as the others. 

```{r fig.width=5, fig.height=5,echo=FALSE}
library(png)

img <- readPNG("/Users/ddbor/Documents/MDA/STATS 9159A (Statistic Modelling I)/Assigments/Project/lambda_path.png")
grid.raster(img)
```
\newpage

## Conclusions

The goal of this project was to fit a multiple linear regression that could be suitable to predict the number of registered users. To get to the results, first, data preprocessing was performed to identify the existing relationship between the variables, where it was discovered some of the predictors were highly correlated. By observing in more detail some plots between the objective variable and some predictors, it was expected that the number of registered users behaved differently according to the season and weather conditions. Before building the model, lasso regression was used to check the significance for each predictor. This led to retain all of the variables and by observing the behavior of the regularization penalty (Figure 4), a model with no penalization would perform better than a penalized model. Because of this, four models were fitted (Table 2) and cross-validation was used to look at the performance. It turned out that the regression model with no interactions or quadratics terms reduced the training error (MSE). The other models were decreasing the performance, which means they were overfitting

After deciding in a model-based in its performance, the collinearity, model assumptions, and influential points were checked. First, the variance inflation value revealed that there was no severe collinearity between the predictor variables. In terms of model assumptions, unfortunately, the model violated the equal variance and normality, by transforming the objective variable using the help of Box-Cox transformation. Despite that, these transformations could not fix the assumptions either adding quadratic terms nor interactions. Subsequently, the influential points and outliers were checked (Figure 7-8). It could not be proved that there were measure errors so they could not be casually removed. In fact, even if they were removed, the model assumptions still were not met.

Finally, looking at the MSE of the test set, it could be concluded that using multiple linear regression was not suitable for the research data. The model chosen had the less mean squared error, but the model assumptions were not met. In addition, because the MAE was small considering the average number of registered users, the predictions using this model were accurate enough.






